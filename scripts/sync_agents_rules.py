#!/usr/bin/env python3
"""Sync AGENTS.md files from Cursor rule definitions.

This script inspects `.cursor/rules/*.mdc` files, reads the YAML-style
front matter to discover the globs that each rule applies to, and renders
standardised Markdown guidance snippets into `AGENTS.md` files within the
matching directories.

Use `--check` to verify the workspace is up to date without writing files.
"""
from __future__ import annotations

import argparse
import ast
import glob
import importlib
import importlib.util
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Mapping, MutableMapping, Sequence

GENERATOR_HEADER = "<!-- Generated by scripts/sync_agents_rules.py; do not edit manually. -->"


@dataclass(frozen=True)
class RuleTemplate:
    slug: str
    title: str
    body: str
    manual_directories: Sequence[str] = ()
    order: int = 0


TEMPLATES: Mapping[str, RuleTemplate] = {
    "agent-artifacts-standard": RuleTemplate(
        slug="agent-artifacts-standard",
        title="Agent Artifacts Standard",
        body=(
            "Ensure every agent artifact stays aligned with the core template."
            " When editing role cards, context packs, playbooks, or checklists,"
            " confirm the document includes the required sections: Purpose &"
            " Scope, Inputs & Outputs, Constraints & Safety, Success Criteria,"
            " Ownership & Escalation, Context Packs, Playbooks, and Version"
            " History. Reference the canonical templates in"
            " `docs/agents/templates/` and keep checklists of required sections"
            " up to date when publishing changes."
        ),
        order=10,
    ),
    "agent-session-notes": RuleTemplate(
        slug="agent-session-notes",
        title="Session Note Required",
        body=(
            "Any session that edits PRDs, agent documentation, or global"
            " standards must leave a session note. Capture decisions, outputs,"
            " follow-ups, and link to impacted files. Save the note in"
            " `docs/agents/session-notes/` and make sure it is committed with"
            " the related work."
        ),
        order=20,
    ),
    "ticket-wizard": RuleTemplate(
        slug="ticket-wizard",
        title="Ticket Wizard Workflow",
        body=(
            "When creating or updating Linear tickets, follow the Ticket Wizard"
            " conversation flow. Confirm the request, gather context, and"
            " tighten the summary before handing it off. Use the ticket"
            " template at `linear/docs/templates/ticket-template.md` and store"
            " drafts under `linear/tickets/drafts/` unless instructed"
            " otherwise. Never push to Linear automatically—share the prepared"
            " output with the user for review."
        ),
        manual_directories=(
            "linear/tickets/drafts",
            "linear/docs/templates",
        ),
        order=30,
    ),
    "planning-mode-md": RuleTemplate(
        slug="planning-mode-md",
        title="Generative Planning Mode",
        body=(
            "Use generative planning mode to explore a problem in a dedicated"
            " Markdown document. Create files inside `docs/raw/plans/` named"
            " `YYYY-MM-DD_topic-name.md`, acknowledge activation, and outline"
            " multiple solution paths with clear recommendations. Keep the"
            " brainstorming document focused on analysis—implementation work"
            " happens later."
        ),
        manual_directories=("docs/raw/plans",),
        order=40,
    ),
    "terminal-command-execution": RuleTemplate(
        slug="terminal-command-execution",
        title="Terminal Command Safety",
        body=(
            "Follow the validated terminal safety rules when providing"
            " commands. Run only one logical command per tool call, avoid"
            " chaining with `&&`, `;`, or pipes for control flow, and never"
            " include newlines inside a command string. For multi-step work,"
            " write a script to disk, mark it executable, then run it. Use"
            " `is_background: true` for long-running tasks."
        ),
        manual_directories=("scripts",),
        order=50,
    ),
}

RULES_DIR = Path(__file__).resolve().parents[1] / ".cursor" / "rules"
REPO_ROOT = Path(__file__).resolve().parents[1]


def _parse_scalar(value: str) -> object:
    if not value:
        return None
    lowered = value.lower()
    if lowered in {"true", "false"}:
        return lowered == "true"
    if value.startswith("[") and value.endswith("]"):
        try:
            return list(ast.literal_eval(value))
        except (ValueError, SyntaxError):
            return []
    if value.startswith(("'", '"')) and value.endswith(("'", '"')):
        try:
            return ast.literal_eval(value)
        except (ValueError, SyntaxError):
            return value
    return value


def _parse_front_matter_manually(front_matter: str) -> Mapping[str, object]:
    data: Dict[str, object] = {}
    current_key: str | None = None
    for raw_line in front_matter.splitlines():
        stripped = raw_line.strip()
        if not stripped or stripped.startswith("#"):
            continue
        if stripped.startswith("-"):
            if current_key is None:
                continue
            existing = data.get(current_key)
            if not isinstance(existing, list):
                continue
            item_value = stripped[1:].strip()
            existing.append(_parse_scalar(item_value))
            continue
        if ":" not in raw_line:
            current_key = None
            continue
        key_part, value_part = raw_line.split(":", 1)
        key = key_part.strip()
        if not key:
            current_key = None
            continue
        value = value_part.strip()
        if not value:
            data[key] = []
            current_key = key
            continue
        parsed = _parse_scalar(value)
        data[key] = parsed
        current_key = key if isinstance(parsed, list) else None
    return data


def parse_front_matter(path: Path) -> Mapping[str, object]:
    text = path.read_text(encoding="utf-8")
    if not text.startswith("---\n"):
        return {}
    try:
        _, remainder = text.split("---\n", 1)
        front_matter, _ = remainder.split("\n---", 1)
    except ValueError:
        return {}

    spec = importlib.util.find_spec("yaml")
    if spec is not None:
        yaml = importlib.import_module("yaml")
        try:
            loaded = yaml.safe_load(front_matter) or {}
        except yaml.YAMLError:
            loaded = {}
        if isinstance(loaded, Mapping):
            return dict(loaded)
    return _parse_front_matter_manually(front_matter)


def resolve_glob(pattern: str) -> Iterable[Path]:
    """Expand a glob pattern and yield directories relative to the repo."""
    matches: List[Path] = []
    for match in glob.glob(str(REPO_ROOT / pattern), recursive=True):
        path = Path(match)
        if path.is_dir():
            matches.append(path)
        else:
            matches.append(path.parent)
    unique: List[Path] = []
    seen = set()
    for path in matches:
        rel = path.relative_to(REPO_ROOT)
        if rel in seen:
            continue
        seen.add(rel)
        unique.append(rel)
    return unique


def gather_rule_targets() -> Mapping[Path, List[RuleTemplate]]:
    directory_rules: MutableMapping[Path, List[RuleTemplate]] = {}
    for slug, template in TEMPLATES.items():
        rule_path = RULES_DIR / f"{slug}.mdc"
        globs: List[str] = []
        if rule_path.exists():
            metadata = parse_front_matter(rule_path)
            raw_globs = metadata.get("globs")
            if isinstance(raw_globs, str):
                globs.append(raw_globs)
            elif isinstance(raw_globs, Sequence):
                globs.extend(str(item) for item in raw_globs)
        globs.extend(template.manual_directories)
        directories: List[Path] = []
        for pattern in globs:
            pattern = pattern.strip()
            if not pattern:
                continue
            if (REPO_ROOT / pattern).exists() and (REPO_ROOT / pattern).is_dir():
                directories.append(Path(pattern))
                continue
            directories.extend(resolve_glob(pattern))
        for directory in directories:
            if directory.name == "":
                continue
            directory_rules.setdefault(directory, []).append(template)
    return directory_rules


def build_expected_contents() -> Dict[Path, str]:
    directory_rules = gather_rule_targets()
    expected: Dict[Path, str] = {}
    for directory, templates in directory_rules.items():
        sorted_templates = sorted(templates, key=lambda t: (t.order, t.title))
        lines: List[str] = [GENERATOR_HEADER, "", "# Agent Guidelines"]
        for template in sorted_templates:
            lines.extend(["", f"## {template.title}", "", template.body.strip()])
        lines.append("")
        content = "\n".join(lines)
        expected[(REPO_ROOT / directory / "AGENTS.md").resolve()] = content
    return expected


def find_existing_generated_files() -> List[Path]:
    results: List[Path] = []
    for path in REPO_ROOT.rglob("AGENTS.md"):
        try:
            if GENERATOR_HEADER in path.read_text(encoding="utf-8"):
                results.append(path.resolve())
        except OSError:
            continue
    return results


def write_files(expected: Mapping[Path, str]) -> bool:
    changed = False
    for path, content in expected.items():
        path.parent.mkdir(parents=True, exist_ok=True)
        existing = path.read_text(encoding="utf-8") if path.exists() else None
        if existing == content:
            continue
        path.write_text(content, encoding="utf-8")
        changed = True
    existing_generated = set(find_existing_generated_files())
    desired_paths = set(expected.keys())
    for path in existing_generated - desired_paths:
        path.unlink()
        changed = True
    return changed


def check_files(expected: Mapping[Path, str]) -> bool:
    clean = True
    desired_paths = set(expected.keys())
    existing_generated = set(find_existing_generated_files())
    for path, content in expected.items():
        if not path.exists():
            print(f"Missing generated file: {path.relative_to(REPO_ROOT)}")
            clean = False
            continue
        existing = path.read_text(encoding="utf-8")
        if existing != content:
            print(f"Outdated generated file: {path.relative_to(REPO_ROOT)}")
            clean = False
    for path in existing_generated - desired_paths:
        print(f"Stale generated file: {path.relative_to(REPO_ROOT)}")
        clean = False
    return clean


def main() -> int:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--check",
        action="store_true",
        help="Verify AGENTS.md files are up to date without writing",
    )
    args = parser.parse_args()

    expected = build_expected_contents()
    if args.check:
        clean = check_files(expected)
        return 0 if clean else 1
    changed = write_files(expected)
    return 0 if not changed else 0


if __name__ == "__main__":
    raise SystemExit(main())
